 @inproceedings{Akyon_Altinuc_Temizel_2022, title={Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection}, url={http://arxiv.org/abs/2202.06934}, DOI={10.1109/ICIP46576.2022.9897990}, abstractNote={Detection of small objects and objects far away in the scene is a major challenge in surveillance applications. Such objects are represented by small number of pixels in the image and lack sufficient details, making them difficult to detect using conventional detectors. In this work, an open-source framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides a generic slicing aided inference and fine-tuning pipeline for small object detection. The proposed technique is generic in the sense that it can be applied on top of any available object detector without any fine-tuning. Experimental evaluations, using object detection baselines on the Visdrone and xView aerial object detection datasets show that the proposed inference method can increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and TOOD detectors, respectively. Moreover, the detection accuracy can be further increased with a slicing aided fine-tuning, resulting in a cumulative increase of 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly available at https://github.com/obss/sahi.git .}, note={arXiv:2202.06934 [cs]}, booktitle={2022 IEEE International Conference on Image Processing (ICIP)}, author={Akyon, Fatih Cagatay and Altinuc, Sinan Onur and Temizel, Alptekin}, year={2022}, month=oct, pages={966–970}, language={en} }
 @article{Feng_Zhong_Jie_Xie_Ma_2024, title={InstaGen: Enhancing Object Detection by Training on Synthetic Dataset}, url={http://arxiv.org/abs/2402.05937}, abstractNote={In this paper, we present a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pretrained, generative diffusion model, to augment it with the ability of localising instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough experiments to show that, this enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer, to enhance object detectors by training on its generated samples, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 ∼ 5.2 AP) scenarios.}, note={arXiv:2402.05937 [cs]}, number={arXiv:2402.05937}, publisher={arXiv}, author={Feng, Chengjian and Zhong, Yujie and Jie, Zequn and Xie, Weidi and Ma, Lin}, year={2024}, month=apr, language={en} }
 @article{Ge_Liu_Wang_Li_Sun_2021, title={YOLOX: Exceeding YOLO Series in 2021}, url={http://arxiv.org/abs/2107.08430}, abstractNote={In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.}, note={arXiv:2107.08430 [cs]}, number={arXiv:2107.08430}, publisher={arXiv}, author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian}, year={2021}, month=aug, language={en} }
 @article{Lin_Maire_Belongie_Bourdev_Girshick_Hays_Perona_Ramanan_Zitnick_Dollár_2015, title={Microsoft COCO: Common Objects in Context}, url={http://arxiv.org/abs/1405.0312}, abstractNote={We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.}, note={arXiv:1405.0312 [cs]}, number={arXiv:1405.0312}, publisher={arXiv}, author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr}, year={2015}, month=feb, language={en} }
 @article{Mumuni_Mumuni_2022, title={Data augmentation: A comprehensive survey of modern approaches}, volume={16}, ISSN={25900056}, DOI={10.1016/j.array.2022.100258}, abstractNote={To ensure good performance, modern machine learning models typically require large amounts of quality annotated data. Meanwhile, the data collection and annotation processes are usually performed manually, and consume a lot of time and resources. The quality and representativeness of curated data for a given task is usually dictated by the natural availability of clean data in the particular domain as well as the level of expertise of developers involved. In many real-world application settings it is often not feasible to obtain sufficient training data. Currently, data augmentation is the most effective way of alleviating this problem. The main goal of data augmentation is to increase the volume, quality and diversity of training data. This paper presents an extensive and thorough review of data augmentation methods applicable in computer vision domains. The focus is on more recent and advanced data augmentation techniques. The surveyed methods include deeply learned augmentation strategies as well as feature-level and meta-learningbased data augmentation techniques. Data synthesis approaches based on realistic 3D graphics modeling, neural rendering, and generative adversarial networks are also covered. Different from previous surveys, we cover a more extensive array of modern techniques and applications. We also compare the performance of several stateof-the-art augmentation methods and present a rigorous discussion of the effectiveness of various techniques in different scenarios of use based on performance results on different datasets and tasks.}, journal={Array}, author={Mumuni, Alhassan and Mumuni, Fuseini}, year={2022}, month=dec, pages={100258}, language={en} }
 @article{Raza_Prokopova_Huseynzade_Azimi_Lafond_2022, title={SimuShips -- A High Resolution Simulation Dataset for Ship Detection with Precise Annotations}, url={http://arxiv.org/abs/2211.05237}, abstractNote={Obstacle detection is a fundamental capability of an autonomous maritime surface vessel (AMSV). State-of-the-art obstacle detection algorithms are based on convolutional neural networks (CNNs). While CNNs provide higher detection accuracy and fast detection speed, they require enormous amounts of data for their training. In particular, the availability of domainspeciﬁc datasets is a challenge for obstacle detection. The difﬁculty in conducting onsite experiments limits the collection of maritime datasets. Owing to the logistic cost of conducting on-site operations, simulation tools provide a safe and costefﬁcient alternative for data collection. In this work, we introduce SimuShips, a publicly available simulation-based dataset for maritime environments. Our dataset consists of 9471 high-resolution (1920x1080) images which include a wide range of obstacle types, atmospheric and illumination conditions along with occlusion, scale and visible proportion variations. We provide annotations in the form of bounding boxes. In addition, we conduct experiments with YOLOv5 to test the viability of simulation data. Our experiments indicate that the combination of real and simulated images improves the recall for all classes by 2.9%.}, note={arXiv:2211.05237 [cs, eess]}, number={arXiv:2211.05237}, publisher={arXiv}, author={Raza, Minahil and Prokopova, Hanna and Huseynzade, Samir and Azimi, Sepinoud and Lafond, Sebastien}, year={2022}, month=sep, language={en} }
 @article{Redmon_Divvala_Girshick_Farhadi_2016, title={You Only Look Once: Unified, Real-Time Object Detection}, url={http://arxiv.org/abs/1506.02640}, abstractNote={We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.}, note={arXiv:1506.02640 [cs]}, number={arXiv:1506.02640}, publisher={arXiv}, author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali}, year={2016}, month=may, language={en} }
 @article{Redmon_Farhadi_2018, title={YOLOv3: An Incremental Improvement}, url={http://arxiv.org/abs/1804.02767}, abstractNote={We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.}, note={arXiv:1804.02767 [cs]}, number={arXiv:1804.02767}, publisher={arXiv}, author={Redmon, Joseph and Farhadi, Ali}, year={2018}, month=apr, language={en} }
 @article{Tan_Huangfu_Wu_Chen_2021, title={Comparison of YOLO v3, Faster R-CNN, and SSD for Real-Time Pill Identification}, rights={https://creativecommons.org/licenses/by/4.0/}, url={https://www.researchsquare.com/article/rs-668895/v1}, DOI={10.21203/rs.3.rs-668895/v1}, abstractNote={Background: The correct identification of pills is very important to ensure the safe administration of drugs to patients. We used three currently mainstream object detection models, respectively Faster R-CNN, Single Shot Multi-Box Detector (SSD), and You Only Look Once v3(YOLO v3), to identify pills and compare the associated performance.
Methods: In this paper, we introduce the basic principles of three object detection models. We trained each algorithm on a pill image dataset and analyzed the performance of the three models to determine the best pill recognition model. Finally, these models are then used to detect difficult samples and compare the results.
Results: The mean average precision (MAP) of Faster R-CNN reached 87.69% but YOLO v3 had a significant advantage in detection speed where the frames per second (FPS) was more than eight times than that of Faster R-CNN. This means that YOLO v3 can operate in real time with a high MAP of 80.17%. The YOLO v3 algorithm also performed better in the comparison of difficult sample detection results. In contrast, SSD did not achieve the highest score in terms of MAP or FPS.
Conclusion: Our study shows that YOLO v3 has advantages in detection speed while maintaining certain MAP and thus can be applied for real-time pill identification in a hospital pharmacy environment.}, author={Tan, Lu and Huangfu, Tianran and Wu, Liyao and Chen, Wenying}, year={2021}, month=jul, language={en} }
 @inproceedings{Unel_Ozkalayci_Cigla_2019, address={Long Beach, CA, USA}, title={The Power of Tiling for Small Object Detection}, rights={https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}, ISBN={978-1-72812-506-0}, url={https://ieeexplore.ieee.org/document/9025422/}, DOI={10.1109/CVPRW.2019.00084}, abstractNote={Deep neural network based techniques are state-of-theart for object detection and classiﬁcation with the help of the development in computational power and memory efﬁciency. Although these networks are adapted for mobile platforms with sacriﬁce in accuracy; the resolution increase in visual sources makes the problem even harder by raising the expectations to leverage all the details in images. Realtime small object detection in low power mobile devices has been one of the fundamental problems of surveillance applications. In this study, we address the detection of pedestrians and vehicles onboard a micro aerial vehicle (MAV) with high-resolution imagery. For this purpose, we exploit PeleeNet, to our best knowledge the most efﬁcient network model on mobile GPUs, as the backbone of an SSD network as well as 38x38 feature map in the earlier layer. After illustrating the low accuracy of state-of-the-art object detectors under the MAV scenario, we introduce a tiling based approach that is applied in both training and inference phases. The proposed technique limits the detail loss in object detection while feeding the network with a ﬁxed size input. The improvements provided by the proposed approach are shown by in-depth experiments performed along Nvidia Jetson TX1 and TX2 using the VisDrone2018 dataset.}, booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, publisher={IEEE}, author={Unel, F. Ozge and Ozkalayci, Burak O. and Cigla, Cevahir}, year={2019}, month=jun, pages={582–591}, language={en} }
 @inproceedings{Wang_Mark Liao_Wu_Chen_Hsieh_Yeh_2020, address={Seattle, WA, USA}, title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, rights={https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}, ISBN={978-1-72819-360-1}, url={https://ieeexplore.ieee.org/document/9150780/}, DOI={10.1109/CVPRW50498.2020.00203}, abstractNote={Small object detection has major applications in the fields of UAVs, surveillance, farming and many others. In this work we investigate the performance of state of the art Yolo based object detection models for the task of small object detection as they are one of the most popular and easy to use object detection models. We evaluated YOLOv5 and YOLOX models in this study. We also investigate the effects of slicing aided inference and fine-tuning the model for slicing aided inference. We used the VisDrone2019Det dataset for training and evaluating our models. This dataset is challenging in the sense that most objects are relatively small compared to the image sizes. This work aims to benchmark the YOLOv5 and YOLOX models for small object detection. We have seen that sliced inference increases the AP50 score in all experiments, this effect was greater for the YOLOv5 models compared to the YOLOX models. The effects of sliced fine-tuning and sliced inference combined produced substantial improvement for all models. The highest AP50 score was achieved by the YOLOv5Large model on the VisDrone2019Det test-dev subset with the score being 48.8.}, booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, publisher={IEEE}, author={Wang, Chien-Yao and Mark Liao, Hong-Yuan and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei and Yeh, I-Hau}, year={2020}, month=jun, pages={1571–1580}, language={en} }
 @article{Xu_Tian_2015, title={A Comprehensive Survey of Clustering Algorithms}, volume={2}, ISSN={2198-5804, 2198-5812}, DOI={10.1007/s40745-015-0040-1}, abstractNote={Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a signiﬁcant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the deﬁnition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.}, number={2}, journal={Annals of Data Science}, author={Xu, Dongkuan and Tian, Yingjie}, year={2015}, month=jun, pages={165–193}, language={en} }
 @article{Yin_Aryani_Petrie_Nambissan_Astudillo_Cao_2024, title={A Rapid Review of Clustering Algorithms}, url={http://arxiv.org/abs/2401.07389}, abstractNote={Clustering algorithms aim to organize data into groups or clusters based on the inherent patterns and similarities within the data. They play an important role in today’s life, such as in marketing and e-commerce, healthcare, data organization and analysis, and social media. Numerous clustering algorithms exist, with ongoing developments introducing new ones. Each algorithm possesses its own set of strengths and weaknesses, and as of now, there is no universally applicable algorithm for all tasks. In this work, we analyzed existing clustering algorithms and classify mainstream algorithms across five different dimensions: underlying principles and characteristics, data point assignment to clusters, dataset capacity, predefined cluster numbers and application area. This classification facilitates researchers in understanding clustering algorithms from various perspectives and helps them identify algorithms suitable for solving specific tasks. Finally, we discussed the current trends and potential future directions in clustering algorithms. We also identified and discussed open challenges and unresolved issues in the field.}, note={arXiv:2401.07389 [cs]}, number={arXiv:2401.07389}, publisher={arXiv}, author={Yin, Hui and Aryani, Amir and Petrie, Stephen and Nambissan, Aishwarya and Astudillo, Aland and Cao, Shengyuan}, year={2024}, month=jan, language={en} }
 @article{Zhang_Cisse_Dauphin_Lopez-Paz_2018, title={mixup: Beyond Empirical Risk Minimization}, url={http://arxiv.org/abs/1710.09412}, abstractNote={Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also ﬁnd that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.}, note={arXiv:1710.09412 [cs, stat]}, number={arXiv:1710.09412}, publisher={arXiv}, author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David}, year={2018}, month=apr, language={en} }
 @article{Zhang_Sun_Jiang_Yu_Weng_Yuan_Luo_Liu_Wang_2022, title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box}, url={http://arxiv.org/abs/2110.06864}, abstractNote={Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and ﬁlter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-theart performance of MOT, we design a simple and strong tracker, named ByteTrack. For the ﬁrst time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack.}, note={arXiv:2110.06864 [cs]}, number={arXiv:2110.06864}, publisher={arXiv}, author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang}, year={2022}, month=apr, language={en} }
 @inproceedings{Žust_Perš_Kristan_2023, address={Paris, France}, title={LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark}, rights={https://doi.org/10.15223/policy-029}, ISBN={9798350307184}, url={https://ieeexplore.ieee.org/document/10377042/}, DOI={10.1109/ICCV51070.2023.01857}, booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, publisher={IEEE}, author={Žust, Lojze and Perš, Janez and Kristan, Matej}, year={2023}, month=oct, pages={20247–20257}, language={en} }
 @article{How to Handle Image of Different Sizes, url={https://wandb.ai/ayush-thakur/dl-question-bank/reports/How-to-Handle-Images-of-Different-Sizes-in-a-Convolutional-Neural-Network--VmlldzoyMDk3NzQ} }

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

 @article{Terven_Cordova-Esparza_Ramirez-Pedraza_Chavez-Urbiola_2023, title={Loss Functions and Metrics in Deep Learning}, url={http://arxiv.org/abs/2307.02694}, abstractNote={One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.}, note={arXiv:2307.02694 [cs]}, number={arXiv:2307.02694}, publisher={arXiv}, author={Terven, Juan and Cordova-Esparza, Diana M. and Ramirez-Pedraza, Alfonso and Chavez-Urbiola, Edgar A.}, year={2023}, month=sep, language={en} }
